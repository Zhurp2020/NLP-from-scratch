{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38164bit34ef27fa136f4cd3bd679e2aa84ca1a2",
   "display_name": "Python 3.8.1 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing with Python\n",
    "## Chapter 3 Processing Raw Text\n",
    "### 3.1 Accessing Text from the Web and from Disk\n",
    "#### Electronic books\n",
    "#### Dealing with HTML\n",
    "#### Processing Search Engine Results\n",
    "#### Processing RSS feeds\n",
    "#### Reading local files\n",
    "#### Extracting Text from PDF, MSWord and other Binary Formats\n",
    "#### Capturing User Input\n",
    "#### The NLP Pipeline\n",
    "### 3.2 Strings: Text Processing at the Lowest Level\n",
    "#### Basic Operations with Strings\n",
    "#### Printing Strings\n",
    "#### Accessing Individual Characters\n",
    "#### Accessing Substrings\n",
    "#### More operations on strings\n",
    "#### The Difference between Lists and Strings\n",
    "### 3.3 Text Processing with Unicode\n",
    "#### What is Unicode?\n",
    "#### Extracting encoded text from files\n",
    "#### Using your local encoding in Python\n",
    "### 3.4 Regular Expressions for Detecting Word Patterns\n",
    "#### Using Basic Meta-Characters\n",
    "#### Ranges and Closures\n",
    "### 3.5 Useful Applications of Regular Expressions\n",
    "#### Extracting Word Pieces\n",
    "#### Doing More with Word Pieces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "a   e   i   o   u \nk 418 148  94 420 173 \np  83  31 105  34  51 \nr 187  63  84  89  79 \ns   0   0 100   2   1 \nt  47   8   0 148  37 \nv  93  27 105  48  49 \n"
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "words = nltk.corpus.toolbox.words('rotokas.dic')\n",
    "CosVow = [j for i in words for j in re.findall(r'[ptksvr][aeiou]',i) ]\n",
    "CosVowCFD = nltk.ConditionalFreqDist(CosVow)\n",
    "CosVowCFD.tabulate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['kaakuupato',\n 'kaapa',\n 'kaapiepato',\n 'kaapopato',\n 'kaepaa',\n 'Kakarapaia',\n 'kakupaa',\n 'kakuparei',\n 'kakupato',\n 'kapa']"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "CosVowWord = [(cv,w)  for w in words for cv in re.findall(r'[ptksvr][aeiou]',w)]\n",
    "CosVowIndex = nltk.Index(CosVowWord)\n",
    "CosVowIndex['pa']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding Word Stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['DENNIS', ':', 'Listen', ',', 'strange', 'women', 'ly', 'in', 'pond', 'distribut', 'sword', 'i', 'no', 'basi', 'for', 'a', 'system', 'of', 'govern', '.', 'Supreme', 'execut', 'power', 'deriv', 'from', 'a', 'mandate', 'from', 'the', 'mass', ',', 'not', 'from', 'some', 'farcical', 'aquatic', 'ceremon', '.']\n"
    }
   ],
   "source": [
    "def stem(word):\n",
    "    StemPattern = r'^(.*?)(ing|ly|ed|y|ious|ies|ive|es|s|ment)?$'\n",
    "    stem,s = re.findall(StemPattern,word)[0]\n",
    "    return stem\n",
    "raw = \"\"\"\n",
    "DENNIS: Listen, strange women lying in ponds distributing swords\n",
    "is no basis for a system of government.  Supreme executive power derives from\n",
    "a mandate from the masses, not from some farcical aquatic ceremony.\n",
    "\"\"\"\n",
    "StemWords = nltk.word_tokenize(raw)\n",
    "print([stem(i) for i in StemWords])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Searching Tokenized Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "monied; nervous; dangerous; white; white; white; pious; queer; good;\nmature; white; Cape; great; wise; wise; butterless; white; fiendish;\npale; furious; better; certain; complete; dismasted; younger; brave;\nbrave; brave; brave\n"
    }
   ],
   "source": [
    "from nltk.corpus import gutenberg,nps_chat\n",
    "moby = nltk.Text(gutenberg.words('melville-moby_dick.txt'))\n",
    "moby.findall(r'<a>(<.*>)<man>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Normalizing Text\n",
    "#### Stemmers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['denni', ':', 'listen', ',', 'strang', 'women', 'lie', 'in', 'pond', 'distribut', 'sword', 'is', 'no', 'basi', 'for', 'a', 'system', 'of', 'govern', '.', 'suprem', 'execut', 'power', 'deriv', 'from', 'a', 'mandat', 'from', 'the', 'mass', ',', 'not', 'from', 'some', 'farcic', 'aquat', 'ceremoni', '.']\n['den', ':', 'list', ',', 'strange', 'wom', 'lying', 'in', 'pond', 'distribut', 'sword', 'is', 'no', 'bas', 'for', 'a', 'system', 'of', 'govern', '.', 'suprem', 'execut', 'pow', 'der', 'from', 'a', 'mand', 'from', 'the', 'mass', ',', 'not', 'from', 'som', 'farc', 'aqu', 'ceremony', '.']\n"
    }
   ],
   "source": [
    "porter = nltk.PorterStemmer()\n",
    "lancaster = nltk.LancasterStemmer()\n",
    "print([porter.stem(i) for i in StemWords])\n",
    "print([lancaster.stem(i) for i in StemWords])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "r king ! DENNIS : Listen , strange women lying in ponds distributing swords is no\n beat a very brave retreat . ROBIN : All lies ! MINSTREL : [ singing ] Bravest of\n       Nay . Nay . Come . Come . You may lie here . Oh , but you are wounded !   \ndoctors immediately ! No , no , please ! Lie down . [ clap clap ] PIGLET : Well  \nere is much danger , for beyond the cave lies the Gorge of Eternal Peril , which \n   you . Oh ... TIM : To the north there lies a cave -- the cave of Caerbannog --\nh it and lived ! Bones of full fifty men lie strewn about its lair . So , brave k\nnot stop our fight ' til each one of you lies dead , and the Holy Grail returns t\n"
    }
   ],
   "source": [
    "class IndexedText(object):\n",
    "    def __init__(self,_text,_stemmer):\n",
    "        self._text = _text\n",
    "        self._stemmer = _stemmer\n",
    "        self._index = nltk.Index((self._stem(word),i) for (i,word) in enumerate(self._text))\n",
    "    def _stem(self,word):\n",
    "        return self._stemmer.stem(word).lower()\n",
    "    def concordance(self,word,width=40):\n",
    "        key = self._stem(word)\n",
    "        WordCount = int(width/4)\n",
    "        for i in self._index[word]:\n",
    "            lcontext = ' '.join(self._text[i-WordCount:i])\n",
    "            rcontext = ' '.join(self._text[i:i+WordCount])\n",
    "            ldisplay = '{:>{width}}'.format(lcontext[-width:],width=width)\n",
    "            rdisplay = '{:{width}}'.format(rcontext[:width],width=width)\n",
    "            print(ldisplay,rdisplay)\n",
    "stemmer = nltk.PorterStemmer()\n",
    "grail = nltk.corpus.webtext.words('grail.txt')\n",
    "text = IndexedText(grail,stemmer)\n",
    "text.concordance('lie')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['DENNIS', ':', 'Listen', ',', 'strange', 'woman', 'lying', 'in', 'pond', 'distributing', 'sword', 'is', 'no', 'basis', 'for', 'a', 'system', 'of', 'government', '.', 'Supreme', 'executive', 'power', 'derives', 'from', 'a', 'mandate', 'from', 'the', 'mass', ',', 'not', 'from', 'some', 'farcical', 'aquatic', 'ceremony', '.']\n"
    }
   ],
   "source": [
    "WordNetLemma = nltk.WordNetLemmatizer()\n",
    "print([WordNetLemma.lemmatize(i) for i in StemWords])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7 Regular Expressions for Tokenizing Text\n",
    "#### Simple Approaches to Tokenization\n",
    "#### NLTK's Regular Expression Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['That', 'U.S.A.', 'poster-print', 'costs', '$12.40', '...']"
     },
     "metadata": {},
     "execution_count": 39
    }
   ],
   "source": [
    "text = 'That U.S.A. poster-print costs $12.40...'\n",
    "pattern = r'(?:[A-Z]\\.)+|\\w+(?:-\\w+)*|\\$?\\d+(?:.\\d+)?%?|\\.\\.\\.|[][,.:\"\\'?():-_`]'\n",
    "nltk.regexp_tokenize(text,pattern)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Further Issues with Tokenization\n",
    "### 3.8 Segmentation\n",
    "#### Sentence Segmentation\n",
    "#### Word Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "['do', 'you', 'see', 'the', 'kitty', 'see', 'the', 'doggy', 'do', 'you', 'like', 'the', 'kitty', 'like', 'the', 'doggy']\n"
    }
   ],
   "source": [
    "def segment(text,segstr):\n",
    "    result = []\n",
    "    last = 0\n",
    "    for i in range(len(segstr)):\n",
    "        if segstr[i] == '1' :\n",
    "            result.append(text[last:i+1])\n",
    "            last = i+1\n",
    "    result.append(text[last:])        \n",
    "    return result\n",
    "text = \"doyouseethekittyseethedoggydoyoulikethekittylikethedoggy\"\n",
    "seg1 = \"0100100100100001001001000010100100010010000100010010000\"\n",
    "print(segment(text,seg1))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "48"
     },
     "metadata": {},
     "execution_count": 69
    }
   ],
   "source": [
    "def evaluate(text,segstr):\n",
    "    WordList = segment(text,segstr)\n",
    "    lexicon = sum([len(i)+1 for i in set(WordList)])\n",
    "    devriation = len(WordList)\n",
    "    return lexicon + devriation\n",
    "evaluate(text,seg1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "64 ['doyouseethekitty', 'seethedoggy', 'doyoulikethekitty', 'likethedoggy']\n64 ['doyouseethekitty', 'seethedoggy', 'doyoulikethekitty', 'likethedoggy']\n64 ['doyouseethekitty', 'seethedoggy', 'doyoulikethekitty', 'likethedoggy']\n64 ['doyouseethekitty', 'seethedoggy', 'doyoulikethekitty', 'likethedoggy']\n64 ['doyouseethekitty', 'seethedoggy', 'doyoulikethekitty', 'likethedoggy']\n64 ['doyouseethekitty', 'seethedoggy', 'doyoulikethekitty', 'likethedoggy']\n64 ['doyouseethekitty', 'seethedoggy', 'doyoulikethekitty', 'likethedoggy']\n64 ['doyouseethekitty', 'seethedoggy', 'doyoulikethekitty', 'likethedoggy']\n64 ['doyouseethekitty', 'seethedoggy', 'doyoulikethekitty', 'likethedoggy']\n64 ['doyouseethekitty', 'seethedoggy', 'doyoulikethekitty', 'likethedoggy']\n64 ['doyouseethekitty', 'seethedoggy', 'doyoulikethekitty', 'likethedoggy']\n64 ['doyouseethekitty', 'seethedoggy', 'doyoulikethekitty', 'likethedoggy']\n64 ['doyouseethekitty', 'seethedoggy', 'doyoulikethekitty', 'likethedoggy']\n64 ['doyouseethekitty', 'seethedoggy', 'doyoulikethekitty', 'likethedoggy']\n64 ['doyouseethekitty', 'seethedoggy', 'doyoulikethekitty', 'likethedoggy']\n64 ['doyouseethekitty', 'seethedoggy', 'doyoulikethekitty', 'likethedoggy']\n64 ['doyouseethekitty', 'seethedoggy', 'doyoulikethekitty', 'likethedoggy']\n64 ['doyouseethekitty', 'seethedoggy', 'doyoulikethekitty', 'likethedoggy']\n64 ['doyouseethekitty', 'seethedoggy', 'doyoulikethekitty', 'likethedoggy']\n64 ['doyouseethekitty', 'seethedoggy', 'doyoulikethekitty', 'likethedoggy']\n64 ['doyouseethekitty', 'seethedoggy', 'doyoulikethekitty', 'likethedoggy']\n64 ['doyouseethekitty', 'seethedoggy', 'doyoulikethekitty', 'likethedoggy']\n64 ['doyouseethekitty', 'seethedoggy', 'doyoulikethekitty', 'likethedoggy']\n64 ['doyouseethekitty', 'seethedoggy', 'doyoulikethekitty', 'likethedoggy']\n64 ['doyouseethekitty', 'seethedoggy', 'doyoulikethekitty', 'likethedoggy']\n64 ['doyouseethekitty', 'seethedoggy', 'doyoulikethekitty', 'likethedoggy']\n64 ['doyouseethekitty', 'seethedoggy', 'doyoulikethekitty', 'likethedoggy']\n64 ['doyouseethekitty', 'seethedoggy', 'doyoulikethekitty', 'likethedoggy']\n64 ['doyouseethekitty', 'seethedoggy', 'doyoulikethekitty', 'likethedoggy']\n64 ['doyouseethekitty', 'seethedoggy', 'doyoulikethekitty', 'likethedoggy']\n64 ['doyouseethekitty', 'seethedoggy', 'doyoulikethekitty', 'likethedoggy']\n64 ['doyouseethekitty', 'seethedoggy', 'doyoulikethekitty', 'likethedoggy']\n64 ['doyouseethekitty', 'seethedoggy', 'doyoulikethekitty', 'likethedoggy']\n64 ['doyouseethekitty', 'seethedoggy', 'doyoulikethekitty', 'likethedoggy']\n64 ['doyouseethekitty', 'seethedoggy', 'doyoulikethekitty', 'likethedoggy']\n61 ['doyousee', 'thekitty', 'seeth', 'edoggydoyoulike', 'thekitty', 'likethedog', 'gy']\n61 ['doyousee', 'thekitty', 'seeth', 'edoggydoyoulike', 'thekitty', 'likethedog', 'gy']\n61 ['doyousee', 'thekitty', 'seeth', 'edoggydoyoulike', 'thekitty', 'likethedog', 'gy']\n61 ['doyousee', 'thekitty', 'seeth', 'edoggydoyoulike', 'thekitty', 'likethedog', 'gy']\n61 ['doyousee', 'thekitty', 'seeth', 'edoggydoyoulike', 'thekitty', 'likethedog', 'gy']\n61 ['doyousee', 'thekitty', 'seeth', 'edoggydoyoulike', 'thekitty', 'likethedog', 'gy']\n61 ['doyousee', 'thekitty', 'seeth', 'edoggydoyoulike', 'thekitty', 'likethedog', 'gy']\n61 ['doyousee', 'thekitty', 'seeth', 'edoggydoyoulike', 'thekitty', 'likethedog', 'gy']\n58 ['doyousee', 'thekitty', 'seeth', 'edoggy', 'do', 'youlike', 'thekitty', 'liketh', 'edoggy']\n58 ['doyousee', 'thekitty', 'seeth', 'edoggy', 'do', 'youlike', 'thekitty', 'liketh', 'edoggy']\n58 ['doyousee', 'thekitty', 'seeth', 'edoggy', 'do', 'youlike', 'thekitty', 'liketh', 'edoggy']\n56 ['doyousee', 'thekitty', 'seeth', 'edoggy', 'doyoulike', 'thekitty', 'liketh', 'edoggy']\n56 ['doyousee', 'thekitty', 'seeth', 'edoggy', 'doyoulike', 'thekitty', 'liketh', 'edoggy']\n56 ['doyousee', 'thekitty', 'seeth', 'edoggy', 'doyoulike', 'thekitty', 'liketh', 'edoggy']\n56 ['doyousee', 'thekitty', 'seeth', 'edoggy', 'doyoulike', 'thekitty', 'liketh', 'edoggy']\n55 ['doyo', 'usee', 'thekitty', 'seeth', 'edoggy', 'doyo', 'ulike', 'thekitty', 'liketh', 'edoggy']\n55 ['doyo', 'usee', 'thekitty', 'seeth', 'edoggy', 'doyo', 'ulike', 'thekitty', 'liketh', 'edoggy']\n55 ['doyo', 'usee', 'thekitty', 'seeth', 'edoggy', 'doyo', 'ulike', 'thekitty', 'liketh', 'edoggy']\n55 ['doyo', 'usee', 'thekitty', 'seeth', 'edoggy', 'doyo', 'ulike', 'thekitty', 'liketh', 'edoggy']\n55 ['doyo', 'usee', 'thekitty', 'seeth', 'edoggy', 'doyo', 'ulike', 'thekitty', 'liketh', 'edoggy']\n55 ['doyo', 'usee', 'thekitty', 'seeth', 'edoggy', 'doyo', 'ulike', 'thekitty', 'liketh', 'edoggy']\n55 ['doyo', 'usee', 'thekitty', 'seeth', 'edoggy', 'doyo', 'ulike', 'thekitty', 'liketh', 'edoggy']\n55 ['doyo', 'usee', 'thekitty', 'seeth', 'edoggy', 'doyo', 'ulike', 'thekitty', 'liketh', 'edoggy']\n55['doyo', 'usee', 'thekitty', 'seeth', 'edoggy', 'doyo', 'ulike', 'thekitty', 'liketh', 'edoggy']\n55 ['doyo', 'usee', 'thekitty', 'seeth', 'edoggy', 'doyo', 'ulike', 'thekitty', 'liketh', 'edoggy']\n55 ['doyo', 'usee', 'thekitty', 'seeth', 'edoggy', 'doyo', 'ulike', 'thekitty', 'liketh', 'edoggy']\n55 ['doyo', 'usee', 'thekitty', 'seeth', 'edoggy', 'doyo', 'ulike', 'thekitty', 'liketh', 'edoggy']\n55 ['doyo', 'usee', 'thekitty', 'seeth', 'edoggy', 'doyo', 'ulike', 'thekitty', 'liketh', 'edoggy']\n55 ['doyo', 'usee', 'thekitty', 'seeth', 'edoggy', 'doyo', 'ulike', 'thekitty', 'liketh', 'edoggy']\n54 ['doyo', 'usee', 'thekitty', 'seeth', 'edoggy', 'doyo', 'u', 'like', 'thekitty', 'like', 'th', 'edoggy']\n49 ['doyo', 'u', 'see', 'thekitty', 'see', 'th', 'edoggy', 'doyo', 'u', 'like', 'thekitty', 'like', 'th', 'edoggy']\n46 ['doyo', 'u', 'see', 'thekitty', 'see', 'thedoggy', 'doyo', 'u', 'like', 'thekitty', 'like', 'thedoggy']\n43 ['doyou', 'see', 'thekitty', 'see', 'thedoggy', 'doyou', 'like', 'thekitty', 'like', 'thedoggy']\n43 ['doyou', 'see', 'thekitty', 'see', 'thedoggy', 'doyou', 'like', 'thekitty', 'like', 'thedoggy']\n43 ['doyou', 'see', 'thekitty', 'see', 'thedoggy', 'doyou', 'like', 'thekitty', 'like', 'thedoggy']\n43 ['doyou', 'see', 'thekitty', 'see', 'thedoggy', 'doyou', 'like', 'thekitty', 'like', 'thedoggy']\n43 ['doyou', 'see', 'thekitty', 'see', 'thedoggy', 'doyou', 'like', 'thekitty', 'like', 'thedoggy']\n43 ['doyou', 'see', 'thekitty', 'see', 'thedoggy', 'doyou', 'like', 'thekitty', 'like', 'thedoggy']\n43 ['doyou', 'see', 'thekitty', 'see', 'thedoggy', 'doyou', 'like', 'thekitty', 'like', 'thedoggy']\n43 ['doyou', 'see', 'thekitty', 'see', 'thedoggy', 'doyou', 'like', 'thekitty', 'like', 'thedoggy']\n43 ['doyou', 'see', 'thekitty', 'see', 'thedoggy', 'doyou', 'like', 'thekitty', 'like', 'thedoggy']\n43 ['doyou', 'see', 'thekitty', 'see', 'thedoggy', 'doyou', 'like', 'thekitty', 'like', 'thedoggy']\n43 ['doyou', 'see', 'thekitty', 'see', 'thedoggy', 'doyou', 'like', 'thekitty', 'like', 'thedoggy']\n43 ['doyou', 'see', 'thekitty', 'see', 'thedoggy', 'doyou', 'like', 'thekitty', 'like', 'thedoggy']\n43 ['doyou', 'see', 'thekitty', 'see', 'thedoggy', 'doyou', 'like', 'thekitty', 'like', 'thedoggy']\n43 ['doyou', 'see', 'thekitty', 'see', 'thedoggy', 'doyou', 'like', 'thekitty', 'like', 'thedoggy']\n43 ['doyou', 'see', 'thekitty', 'see', 'thedoggy', 'doyou', 'like', 'thekitty', 'like', 'thedoggy']\n43 ['doyou', 'see', 'thekitty', 'see', 'thedoggy', 'doyou', 'like', 'thekitty', 'like', 'thedoggy']\n43 ['doyou', 'see', 'thekitty', 'see', 'thedoggy', 'doyou', 'like', 'thekitty', 'like', 'thedoggy']\n43 ['doyou', 'see', 'thekitty', 'see', 'thedoggy', 'doyou', 'like', 'thekitty', 'like', 'thedoggy']\n43 ['doyou', 'see', 'thekitty', 'see', 'thedoggy', 'doyou', 'like', 'thekitty', 'like', 'thedoggy']\n43 ['doyou', 'see', 'thekitty', 'see', 'thedoggy', 'doyou', 'like', 'thekitty', 'like', 'thedoggy']\n43 ['doyou', 'see', 'thekitty', 'see', 'thedoggy', 'doyou', 'like', 'thekitty', 'like', 'thedoggy']\n43 ['doyou', 'see', 'thekitty', 'see', 'thedoggy', 'doyou', 'like', 'thekitty', 'like', 'thedoggy']\n43 ['doyou', 'see', 'thekitty', 'see', 'thedoggy', 'doyou', 'like', 'thekitty', 'like', 'thedoggy']\n43 ['doyou', 'see', 'thekitty', 'see', 'thedoggy', 'doyou', 'like', 'thekitty', 'like', 'thedoggy']\n43 ['doyou', 'see', 'thekitty', 'see', 'thedoggy', 'doyou', 'like', 'thekitty', 'like', 'thedoggy']\n"
    }
   ],
   "source": [
    "from random import randint\n",
    "def FlipStr(string,pos):\n",
    "    return string[:pos] + str(1-int(string[pos])) + string[pos+1:]\n",
    "def CreateNFlip(text,n):\n",
    "    for i in range(n):\n",
    "        text = FlipStr(text,randint(0,len(text)-1))\n",
    "    return text\n",
    "def anneal(text,seg,iteration,CoolRate):\n",
    "    temperature = float(len(seg))\n",
    "    while temperature > 0.5 :\n",
    "        BestScore,BestSeg = evaluate(text,seg),seg\n",
    "        for i in range(iteration):\n",
    "            guess = CreateNFlip(seg,round(temperature))\n",
    "            score = evaluate(text,guess)\n",
    "            if score < BestScore :\n",
    "                BestScore,BestSeg = score,guess\n",
    "        seg = BestSeg\n",
    "        temperature = temperature * CoolRate\n",
    "        print(BestScore,segment(text,BestSeg))\n",
    "seg2 = \"0000000000000001000000000010000000000000000100000000000\"  \n",
    "text2 = '盼望着盼望着东风来了春天的脚步近了' \n",
    "seg3 = '0010001000001000'\n",
    "anneal(text,seg2,1000,0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.9 Formatting: From Lists to Strings\n",
    "#### From Lists to Strings\n",
    "#### Strings and Formats\n",
    "#### Lining Things Up\n",
    "#### Writing Results to a File\n",
    "#### Text Wrapping\n",
    "### 3.10 Summary\n",
    "### 3.11 Further Reading\n",
    "### 3.12 Exercises\n",
    "#### 1. Define a string `s = 'colorless'`. Write a Python statement that changes this to \"colourless\" using only the slice and concatenation operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'colourless'"
     },
     "metadata": {},
     "execution_count": 99
    }
   ],
   "source": [
    "s = 'colorless'\n",
    "s[:4]+'u'+s[4:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. We can use the slice notation to remove morphological endings on words. For example, `'dogs'[:-1]` removes the last character of `dogs`, leaving `dog`. Use slice notation to remove the affixes from these words (we've inserted a hyphen to indicate the affix boundary, but omit this from your strings): `dish-es`, `run-ning`, `nation-ality`, `un-do`, `pre-heat`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'dish'"
     },
     "metadata": {},
     "execution_count": 100
    }
   ],
   "source": [
    "'dish-es'[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'do'"
     },
     "metadata": {},
     "execution_count": 101
    }
   ],
   "source": [
    "'un-do'[3:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. We saw how we can generate an `IndexError` by indexing beyond the end of a string. Is it possible to construct an index that goes too far to the left, before the start of the string?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "IndexError",
     "evalue": "list index out of range",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-102-15aecd526316>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "[1,2,3][-5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. We can specify a \"step\" size for the slice. The following returns every second character within the slice: `monty[6:11:2]`. It also works in the reverse direction: `monty[10:5:-2]` Try these for yourself, then experiment with different step values.\n",
    "#### 5. What happens if you ask the interpreter to evaluate `monty[::-1]`? Explain why this is a reasonable result.\n",
    "#### 6. Describe the class of strings matched by the following regular expressions.\n",
    "+ `[a-zA-Z]+`\n",
    "+ `[A-Z][a-z]*`\n",
    "+ `p[aeiou]{,2}t`\n",
    "+ `\\d+(\\.\\d+)?`\n",
    "+ `([^aeiou][aeiou][^aeiou])*`\n",
    "+ `\\w+|[^\\w\\s]+`  \n",
    "\n",
    "Test your answers using `nltk.re_show()`.\n",
    "#### 7. Write regular expressions to match the following classes of strings:\n",
    "##### a. A single determiner (assume that a, an, and the are the only determiners).\n",
    "##### b. An arithmetic expression using integers, addition, and multiplication, such as `2*3+8`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r'\\ban\\b|\\bthe\\b|\\ba\\b'\n",
    "r'(\\d+[\\+\\*]?)+'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Write a utility function that takes a URL as its argument, and returns the contents of the URL, with all HTML markup removed. Use `from urllib import request` and then `request.urlopen('http://nltk.org/').read().decode('utf8')` to access the contents of the URL.\n",
    "#### 9. Save some text into a file `corpus.txt`. Define a function `load(f)` that reads from the file named in its sole argument, and returns a string containing the text of the file.\n",
    "##### a. Use `nltk.regexp_tokenize()` to create a tokenizer that tokenizes the various kinds of punctuation in this text. Use one multi-line regular expression, with inline comments, using the verbose flag `(?x)`.\n",
    "##### b. Use `nltk.regexp_tokenize()` to create a tokenizer that tokenizes the following kinds of expression: monetary amounts; dates; names of people and organizations.\n",
    "#### 10. Rewrite the following loop as a list comprehension:\n",
    "```\n",
    "sent = ['The', 'dog', 'gave', 'John', 'the', 'newspaper']\n",
    "result = []\n",
    "for word in sent:\n",
    "    word_len = (word, len(word))\n",
    "    result.append(word_len)\n",
    "result\n",
    "[('The', 3), ('dog', 3), ('gave', 4), ('John', 4), ('the', 3), ('newspaper', 9)]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[('The', 3), ('dog', 3), ('gave', 4), ('John', 4), ('the', 3), ('newspaper', 9)]\n"
    }
   ],
   "source": [
    "sent = ['The', 'dog', 'gave', 'John', 'the', 'newspaper']\n",
    "print([(i,len(i)) for i in sent])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11. Define a string `raw`containing a sentence of your own choosing. Now, split `raw` on some character other than space, such as `'s'`.\n",
    "#### 12. Write a `for` loop to print out the characters of a string, one per line.\n",
    "#### 13. What is the difference between calling `split` on a string with no argument or with `' '` as the argument, e.g. `sent.split()` versus `sent.split(' ')`? What happens when the string being split contains tab characters, consecutive space characters, or a sequence of tabs and spaces? (In IDLE you will need to use `'\\t'` to enter a tab character.)\n",
    "#### 14. Create a variable `words` containing a list of words. Experiment with `words.sort()` and `sorted(words)`. What is the difference?\n",
    "#### 15. Explore the difference between strings and integers by typing the following at a Python prompt: `\"3\" * 7` and `3 * 7`. Try converting between strings and integers using `int(\"3\")` and `str(3)`.\n",
    "#### 16. Use a text editor to create a file called `prog.py` containing the single line `monty = 'Monty Python'`. Next, start up a new session with the Python interpreter, and enter the expression `monty` at the prompt. You will get an error from the interpreter. Now, try the following (note that you have to leave off the `.py` part of the filename):\n",
    "'''\n",
    "from prog import monty\n",
    "monty\n",
    "'''\n",
    "This time, Python should return with a value. You can also try `import prog`, in which case Python should be able to evaluate the expression `prog.monty` at the prompt.\n",
    "#### 17. What happens when the formatting strings %6s and %-6s are used to display strings that are longer than six characters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "ddddddd\n"
    }
   ],
   "source": [
    "print('%6s'% 'ddddddd')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 18. Read in some text from a corpus, tokenize it, and print the list of all wh-word types that occur. (wh-words in English are used in questions, relative clauses and exclamations: who, which, what, and so on.) Print them in order. Are any words duplicated in this list, because of the presence of case distinctions or punctuation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "{'What', 'When', 'While', 'Whenever', 'Where', 'Whatever', 'Whether', 'Whoever', 'Which', 'Why', 'Who', 'Whom'}\n"
    }
   ],
   "source": [
    "from nltk.corpus import gutenberg\n",
    "tokens = gutenberg.words('austen-emma.txt')\n",
    "print(set([i for i in tokens if i.startswith('Wh')]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 19. Create a file consisting of words and (made up) frequencies, where each line consists of a word, the space character, and a positive integer, e.g. `fuzzy 53`. Read the file into a Python list using `open(filename).readlines()`. Next, break each line into its two fields using `split()`, and convert the number into an integer using `int()`. The result should be a list of the form: `[['fuzzy', 53], ...]`.\n",
    "#### 20. Write code to access a favorite webpage and extract some text from it. For example, access a weather site and extract the forecast top temperature for your town or city today.\n",
    "#### 21. Write a function `unknown()` that takes a URL as its argument, and returns a list of unknown words that occur on that webpage. In order to do this, extract all substrings consisting of lowercase letters (using `re.findall()`) and remove any items from this set that occur in the Words Corpus (`nltk.corpus.words`). Try to categorize these words manually and discuss your findings.\n",
    "#### 22. Examine the results of processing the URL `http://news.bbc.co.uk/` using the regular expressions suggested above. You will see that there is still a fair amount of non-textual data there, particularly Javascript commands. You may also find that sentence breaks have not been properly preserved. Define further regular expressions that improve the extraction of text from this web page.\n",
    "#### 23. Are you able to write a regular expression to tokenize text in such a way that the word don't is tokenized into do and n't? Explain why this regular expression won't work: «`n't|\\w+`»."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['do', \"n't\"]"
     },
     "metadata": {},
     "execution_count": 132
    }
   ],
   "source": [
    "re.findall(r'n\\'t|\\w+[^(n\\'t)]','don\\'t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 24. Try to write code to convert text into hAck3r, using regular expressions and substitution, where `e` → `3`, `i` → `1`, `o` → `0`, `l` → `|`, `s` → `5`, `.` → `5w33t!`, `ate` → `8`. Normalize the text to lowercase before converting it. Add more substitutions of your own. Now try to map `s` to two different values: `$` for word-initial `s`, and `5` for word-internal `s`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'$31| 055w33t! 8'"
     },
     "metadata": {},
     "execution_count": 191
    }
   ],
   "source": [
    "def convert(string):\n",
    "    result = ''\n",
    "    result = re.sub(r'ate',r'8',string)\n",
    "    result = re.sub(r'e',r'3',result)\n",
    "    result = re.sub(r'o',r'0',result)\n",
    "    result = re.sub(r'i',r'1',result)\n",
    "    result = re.sub(r'l',r'|',result)\n",
    "    result = re.sub(r'\\.',r'5w33t!',result)\n",
    "    result = re.sub(r'\\bs',r'$',result)\n",
    "    result = re.sub(r'\\Bs',r'5',result)\n",
    "    return result\n",
    "convert('seil os. ate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 25. Pig Latin is a simple transformation of English text. Each word of the text is converted as follows: move any consonant (or consonant cluster) that appears at the start of the word to the end, then append ay, e.g. string → ingstray, idle → idleay. `http://en.wikipedia.org/wiki/Pig_Latin`\n",
    "##### a. Write a function to convert a word to Pig Latin.\n",
    "##### b. Write code that converts text, instead of individual words.\n",
    "##### c. Extend it further to preserve capitalization, to keep `qu` together (i.e. so that `quiet` becomes `ietquay`), and to detect when `y` is used as a consonant (e.g. `yellow`) vs a vowel (e.g. `style`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'ylestay'"
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "source": [
    "def convertPL(word):\n",
    "    cos = re.match(r'qu|[^aeiouAEIOUyY]*',word)\n",
    "    return word[cos.span()[1]:] + cos.group() + 'ay'\n",
    "convertPL('style')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 26. Download some text from a language that has vowel harmony (e.g. Hungarian), extract the vowel sequences of words, and create a vowel bigram table.\n",
    "#### 27. Python's `random` module includes a function `choice()` which randomly chooses an item from a sequence, e.g. `choice(\"aehh \")` will produce one of four possible characters, with the letter `h` being twice as frequent as the others. Write a generator expression that produces a sequence of 500 randomly chosen letters drawn from the string \"aehh \", and put this expression inside a call to the `''.join()` function, to concatenate them into one long string. You should get a result that looks like uncontrolled sneezing or maniacal laughter: `he  haha ee  heheeh eha`. Use `split()` and `join()` again to normalize the whitespace in this string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "eaeh aeeh e  hhhaeaahhhea aahhh e hhaeeeahehah  eehehhaaeea eeea a hha hahee hhah ah hhehe h hheheae\n"
    }
   ],
   "source": [
    "import random\n",
    "print(''.join([random.choice('aehh ') for i in range(100)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 28. Consider the numeric expressions in the following sentence from the MedLine Corpus: The corresponding free cortisol fractions in these sera were 4.53 +/- 0.15% and 8.16 +/- 0.23%, respectively. Should we say that the numeric expression 4.53 +/- 0.15% is three words? Or should we say that it's a single compound word? Or should we say that it is actually nine words, since it's read \"four point five three, plus or minus zero point fifteen percent\"? Or should we say that it's not a \"real\" word at all, since it wouldn't appear in any dictionary? Discuss these different possibilities. Can you think of application domains that motivate at least two of these answers?\n",
    "#### 29. Readability measures are used to score the reading difficulty of a text, for the purposes of selecting texts of appropriate difficulty for language learners. Let us define $μ_w$ to be the average number of letters per word, and $μ_s$ to be the average number of words per sentence, in a given text. The Automated Readability Index (ARI) of the text is defined to be: $4.71 μw + 0.5 μs - 21.43$. Compute the ARI score for various sections of the Brown Corpus, including section `f` (lore) and `j` (learned). Make use of the fact that `nltk.corpus.brown.words()` produces a sequence of words, while `nltk.corpus.brown.sents()` produces a sequence of sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "10.254756197101155"
     },
     "metadata": {},
     "execution_count": 151
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "def ari(WordList,SenList):\n",
    "    uw = sum([len(i) for i in WordList])/len(WordList)\n",
    "    us = sum([len(i) for i in SenList])/len(SenList)\n",
    "    return 4.71*uw+0.5*us-21.43\n",
    "ari(brown.words(categories='lore'),brown.sents(categories = 'lore'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 30. Use the Porter Stemmer to normalize some tokenized text, calling the stemmer on each word. Do the same thing with the Lancaster Stemmer and see if you observe any differences.\n",
    "#### 31. Define the variable saying to contain the list `['After', 'all', 'is', 'said', 'and', 'done', ',', 'more', 'is', 'said', 'than', 'done', '.']`. Process this list using a `for` loop, and store the length of each word in a new list `lengths`. Hint: begin by assigning the empty list to `lengths`, using `lengths = []`. Then each time through the loop, use `append()` to add another length value to the list. Now do the same thing using a list comprehension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = ['After', 'all', 'is', 'said', 'and', 'done', ',', 'more', 'is', 'said', 'than', 'done', '.']\n",
    "lengths = [len(i) for i in l]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 32. Define a variable `silly` to contain the string: `'newly formed bland ideas are inexpressible in an infuriating way'`. (This happens to be the legitimate interpretation that bilingual English-Spanish speakers can assign to Chomsky's famous nonsense phrase, colorless green ideas sleep furiously according to Wikipedia). Now write code to perform the following tasks:\n",
    "##### a. Split `silly` into a list of strings, one per word, using Python's `split()` operation, and save this to a variable called `bland`.\n",
    "##### b. Extract the second letter of each word in `silly` and join them into a string, to get `'eoldrnnnna'`.\n",
    "##### c. Combine the words in `bland` back into a single string, using `join()`. Make sure the words in the resulting string are separated with whitespace.\n",
    "##### d. Print the words of `silly` in alphabetical order, one per line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "eoldrnnnna\n['an', 'are', 'bland', 'formed', 'ideas', 'in', 'inexpressible', 'infuriating', 'newly', 'way']\n"
    }
   ],
   "source": [
    "silly = 'newly formed bland ideas are inexpressible in an infuriating way'\n",
    "bland = ''.join([i[1] for i in silly.split()])\n",
    "print(bland)\n",
    "print(sorted(silly.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 33. The `index()` function can be used to look up items in sequences. For example, 'inexpressible'.index('e') tells us the index of the first position of the letter e.\n",
    "##### a. What happens when you look up a substring, e.g. `'inexpressible'.index('re')`?\n",
    "##### b. Define a variable `words` containing a list of words. Now use `words.index()` to look up the position of an individual word.\n",
    "##### c. Define a variable `silly` as in the exercise above. Use the `index()` function in combination with list slicing to build a list `phrase` consisting of all the words up to (but not including) `in` in silly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "5\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'newly formed bland ideas are '"
     },
     "metadata": {},
     "execution_count": 48
    }
   ],
   "source": [
    "print('inexpressible'.index('re'))\n",
    "silly[:silly.index('in')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 34. Write code to convert nationality adjectives like Canadian and Australian to their corresponding nouns Canada and Australia (see `http://en.wikipedia.org/wiki/List_of_adjectival_forms_of_place_names`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'canada'"
     },
     "metadata": {},
     "execution_count": 51
    }
   ],
   "source": [
    "def convertadj(word):\n",
    "    return re.sub(r'ian\\b',r'a',word)\n",
    "convertadj('canadian')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 35. Read the LanguageLog post on phrases of the form as best as p can and as best p can, where p is a pronoun. Investigate this phenomenon with the help of a corpus and the `findall()` method for searching tokenized text described in 3.5. `http://itre.cis.upenn.edu/~myl/languagelog/archives/002733.html`\n",
    "#### 36. Study the lolcat version of the book of Genesis, accessible as `nltk.corpus.genesis.words('lolcat.txt')`, and the rules for converting text into lolspeak at `http://www.lolcatbible.com/index.php?title=How_to_speak_lolcat`. Define regular expressions to convert English words into corresponding lolspeak words.\n",
    "#### 37. Read about the `re.sub()` function for string substitution using regular expressions, using `help(re.sub)` and by consulting the further readings for this chapter. Use `re.sub` in writing code to remove HTML tags from an HTML file, and to normalize whitespace.\n",
    "#### 38. An interesting challenge for tokenization is words that have been split across a line-break. E.g. if long-term is split, then we have the string `long-\\nterm`.\n",
    "##### a. Write a regular expression that identifies words that are hyphenated at a line-break. The expression will need to include the `\\n` character.\n",
    "##### b. Use re.sub() to remove the `\\n` character from these words.\n",
    "##### c. How might you identify words that should not remain hyphenated once the newline is removed, e.g. 'encyclo-\\npedia'?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'long-term'"
     },
     "metadata": {},
     "execution_count": 77
    }
   ],
   "source": [
    "s = 'long-\\nterm'\n",
    "re.sub(r'(\\w+-)(\\n)(\\w+)',r'\\1\\3',s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 39. Read the Wikipedia entry on Soundex. Implement this algorithm in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'T522'"
     },
     "metadata": {},
     "execution_count": 132
    }
   ],
   "source": [
    "def SoundEx(word):\n",
    "    result = word[0]\n",
    "    cos = word.lower()\n",
    "    cos = re.sub(r'([bfpv])+([hw][bfpv]+)?',r'\\1',cos)\n",
    "    cos = re.sub(r'([cgjkqsxz])+([hw][cgjkqsxz]+)?',r'\\1',cos)\n",
    "    cos = re.sub(r'([dt])+([hw][dt]+)?',r'\\1',cos)\n",
    "    cos = re.sub(r'(l)+([hw]l+)?',r'l',cos)\n",
    "    cos = re.sub(r'([mn])+([hw][mn]+)?',r'\\1',cos)\n",
    "    cos = re.sub(r'(r)+([hw]r+)?',r'\\1',cos) \n",
    "    cos = cos[1:]\n",
    "    cos = re.sub(r'[bfpv]',r'1',cos)\n",
    "    cos = re.sub(r'[cgjkqsxz]',r'2',cos)\n",
    "    cos = re.sub(r'[dt]',r'3',cos)\n",
    "    cos = re.sub(r'l',r'4',cos)\n",
    "    cos = re.sub(r'[mn]',r'5',cos)\n",
    "    cos = re.sub(r'r',r'6',cos)\n",
    "    cos = re.sub(r'[aeiouyhw]',r'',cos)\n",
    "    result += cos\n",
    "    if len(result[1:]) < 3 :\n",
    "        result = result[0] + result[1:].ljust(3,'0')\n",
    "    return result[:4]\n",
    "SoundEx('Tymczak')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 40. Obtain raw texts from two or more genres and compute their respective reading difficulty scores as in the earlier exercise on reading difficulty. E.g. compare ABC Rural News and ABC Science News (`nltk.corpus.abc`). Use Punkt to perform sentence segmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "10.66074843699441\n10.703963706930097\n"
    }
   ],
   "source": [
    "text1 = nltk.corpus.abc.raw('rural.txt')\n",
    "TokenList1 = nltk.corpus.abc.words('rural.txt')\n",
    "SentList1 = [i.split() for i in nltk.sent_tokenize(text1)]\n",
    "print(ari(TokenList1,SentList1))\n",
    "text2 = nltk.corpus.abc.raw('science.txt')\n",
    "TokenList2 = nltk.corpus.abc.words('science.txt')\n",
    "SentList2 = [i.split() for i in nltk.sent_tokenize(text2)]\n",
    "print(ari(TokenList2,SentList2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 41. Rewrite the following nested loop as a nested list comprehension:\n",
    "```\n",
    "words = ['attribution', 'confabulation', 'elocution',\n",
    "         'sequoia', 'tenacious', 'unidirectional']\n",
    "vsequences = set()\n",
    "for word in words:\n",
    "    vowels = []\n",
    "    for char in word:\n",
    "        if char in 'aeiou':\n",
    "            vowels.append(char)\n",
    "    vsequences.add(''.join(vowels))\n",
    "sorted(vsequences)\n",
    "['aiuio', 'eaiou', 'eouio', 'euoia', 'oauaio', 'uiieioa']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['aiuio', 'eaiou', 'eouio', 'euoia', 'oauaio', 'uiieioa']"
     },
     "metadata": {},
     "execution_count": 157
    }
   ],
   "source": [
    "words = ['attribution', 'confabulation', 'elocution','sequoia', 'tenacious','unidirectional']\n",
    "sorted([''.join([i for i in word if i in 'aeiou']) for word in words])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 42. Use WordNet to create a semantic index for a text collection. Extend the concordance search program in 3.6, indexing each word using the offset of its first synset, e.g. `wn.synsets('dog')[0].offset` (and optionally the offset of some of its ancestors in the hypernym hierarchy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "class WNIndexedText(IndexedText):\n",
    "    def __init__(self,text,stemmer):\n",
    "        self._text = _text\n",
    "        self._stemmer = _stemmer\n",
    "        self._index = nltk.Index((self._stem(word),wn.synsets(self._stem(word))[0].offset()) for word in self._text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 43. With the help of a multilingual corpus such as the Universal Declaration of Human Rights Corpus (`nltk.corpus.udhr`), and NLTK's frequency distribution and rank correlation functionality (`nltk.FreqDist, nltk.spearman_correlation`), develop a system that guesses the language of a previously unseen text. For simplicity, work with a single character encoding and just a few languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "English-Latin1 0.9751468222795795\nFrench_Francais-Latin1 -2.5911764705882354\nSpanish-Latin1 -6.782967032967033\n"
    }
   ],
   "source": [
    "from nltk.corpus import udhr\n",
    "FileList = ['English-Latin1','French_Francais-Latin1','Spanish-Latin1']\n",
    "LangFreqList = [nltk.FreqDist(udhr.words(i)) for i in FileList]\n",
    "test = nltk.FreqDist(brown.words('ca01'))\n",
    "for i in range(3):\n",
    "    print(FileList[i],nltk.spearman_correlation(test,LangFreqList[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 44. Write a program that processes a text and discovers cases where a word has been used with a novel sense. For each word, compute the WordNet similarity between all synsets of the word and all synsets of the words in its context. (Note that this is a crude approach; doing it well is a difficult, open research problem.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 45. Read the article on normalization of non-standard words (Sproat et al, 2001), and implement a similar system for text normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}