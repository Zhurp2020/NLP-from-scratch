{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1600684412291",
   "display_name": "Python 3.8.1 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Natural Language Processing with Python\n",
    "## Chapter 8 Analyzing Sentence Structure\n",
    "### 1 Some Grammatical Dilemmas\n",
    "#### 1.1 Linguistic Data and Unlimited Possibilities\n",
    "#### 1.2 Ubiquitous Ambiguity"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(S\n  (NP I)\n  (VP\n    (VP (V shot) (NP (Det an) (N elephant)))\n    (PP (P in) (NP (Det my) (N pajamas)))))\n(S\n  (NP I)\n  (VP\n    (V shot)\n    (NP (Det an) (N elephant) (PP (P in) (NP (Det my) (N pajamas))))))\n"
    }
   ],
   "source": [
    "grammarch = nltk.CFG.fromstring(\"\"\"\n",
    "S -> NP VP\n",
    "PP -> P NP\n",
    "NP -> Det N | Det N PP | 'I'\n",
    "VP -> V NP | VP PP\n",
    "Det -> 'an' | 'my'\n",
    "N -> 'elephant' | 'pajamas'\n",
    "V -> 'shot'\n",
    "P -> 'in'\n",
    "\"\"\")\n",
    "sent = ['I', 'shot', 'an', 'elephant', 'in', 'my', 'pajamas']\n",
    "parser = nltk.ChartParser(grammarch)\n",
    "for tree in parser.parse(sent):\n",
    "    print(tree)"
   ]
  },
  {
   "source": [
    "### 2. What's the Use of Syntax?\n",
    "#### 2.1 Beyond n-grams\n",
    "### 3. Context Free Grammar\n",
    "#### 3.1 A Simple Grammar"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(S (NP Mary) (VP (V saw) (NP Bob)))\n"
    }
   ],
   "source": [
    "grammar = nltk.CFG.fromstring('''\n",
    "  S -> NP VP\n",
    "  VP -> V NP | V NP PP\n",
    "  PP -> P NP\n",
    "  V -> \"saw\" | \"ate\" | \"walked\"\n",
    "  NP -> \"John\" | \"Mary\" | \"Bob\" | Det N | Det N PP\n",
    "  Det -> \"a\" | \"an\" | \"the\" | \"my\"\n",
    "  N -> \"man\" | \"dog\" | \"cat\" | \"telescope\" | \"park\"\n",
    "  P -> \"in\" | \"on\" | \"by\" | \"with\"\n",
    "''')\n",
    "rdparser = nltk.RecursiveDescentParser(grammar)\n",
    "sent = 'Mary saw Bob'.split()\n",
    "for tree in rdparser.parse(sent):\n",
    "    print(tree)"
   ]
  },
  {
   "source": [
    "#### 3.2 Writing Your Own Grammars\n",
    "#### 3.3 Recursion in Syntactic Structure\n",
    "### 4. Parsing With Context Free Grammar\n",
    "#### 4.1 Recursive Descent Parsing\n",
    "#### 4.2 Shift-Reduce Parsing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(S (NP Mary) (VP (V saw) (NP Bob)))\n"
    }
   ],
   "source": [
    "srparser = nltk.ShiftReduceParser(grammar)\n",
    "for tree in srparser.parse(sent):\n",
    "    print(tree)"
   ]
  },
  {
   "source": [
    "#### 4.3 The Left-Corner Parser\n",
    "#### 4.4 Well-Formed Substring Tables"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "\nWFST 1    2    3    4    5    6    7   \n0    NP   .    .    .    .    .    .    \n1    .    V    .    .    .    .    .    \n2    .    .    Det  .    .    .    .    \n3    .    .    .    N    .    .    .    \n4    .    .    .    .    P    .    .    \n5    .    .    .    .    .    Det  .    \n6    .    .    .    .    .    .    N    \n[2] Det [3]   N [4] ==> [2]  NP [4]\n[5] Det [6]   N [7] ==> [5]  NP [7]\n[1]   V [2]  NP [4] ==> [1]  VP [4]\n[4]   P [5]  NP [7] ==> [4]  PP [7]\n[0]  NP [1]  VP [4] ==> [0]   S [4]\n[1]  VP [4]  PP [7] ==> [1]  VP [7]\n[0]  NP [1]  VP [7] ==> [0]   S [7]\n\nWFST 1    2    3    4    5    6    7   \n0    NP   .    .    S    .    .    S    \n1    .    V    .    VP   .    .    VP   \n2    .    .    Det  NP   .    .    .    \n3    .    .    .    N    .    .    .    \n4    .    .    .    .    P    .    PP   \n5    .    .    .    .    .    Det  NP   \n6    .    .    .    .    .    .    N    \n"
    }
   ],
   "source": [
    "text = ['I', 'shot', 'an', 'elephant', 'in', 'my', 'pajamas']\n",
    "def InitWFST(tokens,grammar):\n",
    "    TokenLen = len(tokens)\n",
    "    wfst = [[None for i in range(TokenLen+1)] for i in range(TokenLen+1)]\n",
    "    for i in range(TokenLen):\n",
    "        productions = grammar.productions(rhs=tokens[i])\n",
    "        wfst[i][i+1] = productions[0].lhs()\n",
    "    return wfst\n",
    "def CompleteWFST(wfst,tokens,grammar,trace):\n",
    "    index = dict([(p.rhs(),p.lhs()) for p in grammar.productions()])\n",
    "    TokenLen = len(tokens)\n",
    "    for span in range(2,TokenLen+1):\n",
    "        for start in range(TokenLen+1-span):\n",
    "            end = start + span\n",
    "            for mid in range(start+1,end):\n",
    "                nt1,nt2 = wfst[start][mid],wfst[mid][end]\n",
    "                if nt1 and nt2 and (nt1,nt2) in index:\n",
    "                    wfst[start][end] = index[(nt1,nt2)]\n",
    "                    if trace:\n",
    "                        print(\"[%s] %3s [%s] %3s [%s] ==> [%s] %3s [%s]\" % \\\n",
    "                        (start, nt1, mid, nt2, end, start, index[(nt1,nt2)], end))\n",
    "    return wfst\n",
    "def display(wfst,tokens):\n",
    "    print('\\nWFST ' + ' '.join((\"%-4d\" % i) for i in range(1, len(wfst))))\n",
    "    for i in range(len(wfst)-1):\n",
    "        print(\"%d   \" % i, end=\" \")\n",
    "        for j in range(1, len(wfst)):\n",
    "            print(\"%-4s\" % (wfst[i][j] or '.'), end=\" \")\n",
    "        print()\n",
    "wfst = InitWFST(text,grammarch)\n",
    "wfst\n",
    "display(wfst,text)\n",
    "wfst1 = CompleteWFST(wfst,text,grammarch,True)\n",
    "display(wfst1,text)"
   ]
  },
  {
   "source": [
    "### 5 Dependencies and Dependency Grammar\n",
    "#### 5.1 Valency and the Lexicon"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Dependency grammar with 7 productions\n  'shot' -> 'I'\n  'shot' -> 'elephant'\n  'shot' -> 'in'\n  'elephant' -> 'an'\n  'elephant' -> 'in'\n  'in' -> 'pajamas'\n  'pajamas' -> 'my'\n"
    }
   ],
   "source": [
    "grammardep = nltk.DependencyGrammar.fromstring('''\n",
    "'shot' -> 'I' | 'elephant' | 'in'\n",
    "'elephant' -> 'an' | 'in'\n",
    "'in' -> 'pajamas'\n",
    "'pajamas' -> 'my'\n",
    "''')\n",
    "print(grammardep)"
   ]
  },
  {
   "source": [
    "#### 5.2 Scaling Up\n",
    "### 6. Grammar Development\n",
    "#### 6.1 Treebanks and Grammars\n",
    "#### 6.2 Pernicious Ambiguity\n",
    "#### 6.3 Weighted Grammar\n",
    "### 7 Summary\n",
    "### 8 Further Reading\n",
    "### 9 Exercises\n",
    "#### 1. Can you come up with grammatical sentences that have probably never been uttered before? (Take turns with a partner.) What does this tell you about human language?\n",
    "#### 2. Recall Strunk and White's prohibition against sentence-initial however used to mean \"although\". Do a web search for however used at the start of the sentence. How widely used is this construction?\n",
    "#### 3. Consider the sentence Kim arrived or Dana left and everyone cheered. Write down the parenthesized forms to show the relative scope of and and or. Generate tree structures corresponding to both of these interpretations.\n",
    "#### 4. The `Tree` class implements a variety of other useful methods. See the Tree help documentation for more details, i.e. import the Tree class and then type `help(Tree)`.\n",
    "#### 5. In this exercise you will manually construct some parse trees.\n",
    "##### a. Write code to produce two trees, one for each reading of the phrase old men and women"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(NP (NP (ADJ old) (N men)) (COR and) (N women))\n(NP (ADJ old) (NP (N men) (COR and) (N women)))\n"
    }
   ],
   "source": [
    "grammar = nltk.CFG.fromstring('''\n",
    "NP -> ADJ N | NP COR N | N COR N | ADJ NP\n",
    "N -> 'men' | 'women'\n",
    "ADJ -> 'old' \n",
    "COR -> 'and'\n",
    "''')\n",
    "phrase = ['old','men','and','women']\n",
    "parser = nltk.ChartParser(grammar)\n",
    "for tree in parser.parse(phrase):\n",
    "    print(tree)   "
   ]
  },
  {
   "source": [
    "##### b. Encode any of the trees presented in this chapter as a labeled bracketing and use `nltk.Tree()` to check that it is well-formed. Now use `draw()` to display the tree.\n",
    "##### c. As in (a) above, draw a tree for The woman saw a man last Thursday.\n",
    "#### 6. Write a recursive function to traverse a tree and return the depth of the tree, such that a tree with a single node would have depth zero. (Hint: the depth of a subtree is the maximum depth of its children, plus one.)\n",
    "#### 7. Analyze the A.A. Milne sentence about Piglet, by underlining all of the sentences it contains then replacing these with `S` (e.g. the first sentence becomes `S` when:lx` `S`). Draw a tree structure for this \"compressed\" sentence. What are the main syntactic constructions used for building such a long sentence?\n",
    "#### 8. In the recursive descent parser demo, experiment with changing the sentence to be parsed by selecting Edit Text in the Edit menu.\n",
    "#### 9. Can the grammar in `grammar1` be used to describe sentences that are more than 20 words in length?\n",
    "#### 10. Use the graphical chart-parser interface to experiment with different rule invocation strategies. Come up with your own strategy that you can execute manually using the graphical interface. Describe the steps, and report any efficiency improvements it has (e.g. in terms of the size of the resulting chart). Do these improvements depend on the structure of the grammar? What do you think of the prospects for significant performance boosts from cleverer rule invocation strategies?\n",
    "#### 11. With pen and paper, manually trace the execution of a recursive descent parser and a shift-reduce parser, for a CFG you have already seen, or one of your own devising.\n",
    "#### 12. We have seen that a chart parser adds but never removes edges from a chart. Why?\n",
    "#### 13 Consider the sequence of words: Buffalo buffalo Buffalo buffalo buffalo buffalo Buffalo buffalo. This is a grammatically correct sentence, as explained at `http://en.wikipedia.org/wiki/Buffalo_buffalo_Buffalo_buffalo_buffalo_buffalo_Buffalo_buffalo`. Consider the tree diagram presented on this Wikipedia page, and write down a suitable grammar. Normalize case to lowercase, to simulate the problem that a listener has when hearing this sentence. Can you find other parses for this sentence? How does the number of parse trees grow as the sentence gets longer? (More examples of these sentences can be found at `http://en.wikipedia.org/wiki/List_of_homophonous_phrases`).\n",
    "#### 14. You can modify the grammar in the recursive descent parser demo by selecting Edit Grammar in the Edit menu. Change the second expansion production, namely `NP -> Det N PP`, to `NP -> NP`PP`. Using the Step button, try to build a parse tree. What happens?\n",
    "#### 15. Extend the grammar in `grammar2` with productions that expand prepositions as intransitive, transitive and requiring a `PP` complement. Based on these productions, use the method of the preceding exercise to draw a tree for the sentence Lee ran away home.\n",
    "#### 16. Pick some common verbs and complete the following tasks:\n",
    "##### a. Write a program to find those verbs in the Prepositional Phrase Attachment Corpus `nltk.corpus.ppattach`. Find any cases where the same verb exhibits two different attachments, but where the first noun, or second noun, or preposition, stay unchanged (as we saw in our discussion of syntactic ambiguity in 2).\n",
    "##### b. Devise CFG grammar productions to cover some of these cases.\n",
    "#### 17. Write a program to compare the efficiency of a top-down chart parser compared with a recursive descent parser (4). Use the same grammar and input sentences for both. Compare their performance using the `timeit` module (see 4.7 for an example of how to do this).\n",
    "#### 18. Compare the performance of the top-down, bottom-up, and left-corner parsers using the same grammar and three grammatical test sentences. Use `timeit` to log the amount of time each parser takes on the same sentence. Write a function that runs all three parsers on all three sentences, and prints a 3-by-3 grid of times, as well as row and column totals. Discuss your findings.\n",
    "#### 19. Read up on \"garden path\" sentences. How might the computational work of a parser relate to the difficulty humans have with processing these sentences? `http://en.wikipedia.org/wiki/Garden_path_sentence`\n",
    "#### 20. To compare multiple trees in a single window, we can use the `draw_trees()` method. Define some trees and try it out:\n",
    "```\t\n",
    ">>> from nltk.draw.tree import draw_trees\n",
    ">>> draw_trees(tree1, tree2, tree3)              \n",
    "```      \n",
    "#### 21. Using tree positions, list the subjects of the first 100 sentences in the Penn treebank; to make the results easier to view, limit the extracted subjects to subtrees whose height is 2.\n",
    "#### 22. Inspect the Prepositional Phrase Attachment Corpus and try to suggest some factors that influence `PP` attachment.\n",
    "#### 23. In this section we claimed that there are linguistic regularities that cannot be described simply in terms of n-grams. Consider the following sentence, particularly the position of the phrase in his turn. Does this illustrate a problem for an approach based on n-grams?   \n",
    "What was more, the in his turn somewhat youngish Nikolay Parfenovich also turned out to be the only person in the entire world to acquire a sincere liking to our \"discriminated-against\" public procurator. (Dostoevsky: The Brothers Karamazov)\n",
    "#### 24. Write a recursive function that produces a nested bracketing for a tree, leaving out the leaf nodes, and displaying the non-terminal labels after their subtrees. So the above example about Pierre Vinken would produce: `[[[NNP NNP]NP , [ADJP [CD NNS]NP JJ]ADJP ,]NP-SBJ MD [VB [DT NN]NP [IN [DT JJ NN]NP]PP-CLR [NNP CD]NP-TMP]VP .]S` Consecutive categories should be separated by space.\n",
    "#### 25. Download several electronic books from Project Gutenberg. Write a program to scan these texts for any extremely long sentences. What is the longest sentence you can find? What syntactic construction(s) are responsible for such long sentences?\n",
    "#### 26. Modify the functions `init_wfst()` and `complete_wfst()` so that the contents of each cell in the WFST is a set of non-terminal symbols rather than a single non-terminal.\n",
    "#### 27. Consider the algorithm in 4.4. Can you explain why parsing context-free grammar is proportional to n3, where n is the length of the input sentence.\n",
    "#### 28. Process each tree of the Treebank corpus sample `nltk.corpus.treebank` and extract the productions with the help of `Tree.productions()`. Discard the productions that occur only once. Productions with the same left hand side, and similar right hand sides can be collapsed, resulting in an equivalent but more compact set of rules. Write code to output a compact grammar.\n",
    "#### 29. One common way of defining the subject of a sentence `S` in English is as the noun phrase that is the child of `S` and the sibling of `VP`. Write a function that takes the tree for a sentence and returns the subtree corresponding to the subject of the sentence. What should it do if the root node of the tree passed to this function is not `S`, or it lacks a subject?\n",
    "#### 30. Write a function that takes a grammar (such as the one defined in 3.1) and returns a random sentence generated by the grammar. (Use `grammar.start()` to find the start symbol of the grammar; `grammar.productions(lhs)` to get the list of productions from the grammar that have the specified left-hand side; and `production.rhs()` to get the right-hand side of a production.)\n",
    "#### 32. mplement a version of the shift-reduce parser using backtracking, so that it finds all possible parses for a sentence, what might be called a \"recursive ascent parser.\" Consult the Wikipedia entry for backtracking at `http://en.wikipedia.org/wiki/Backtracking`\n",
    "#### 34. As we saw in 7., it is possible to collapse chunks down to their chunk label. When we do this for sentences involving the word gave, we find patterns such as the following:\n",
    "```\n",
    "gave NP\n",
    "gave up NP in NP\n",
    "gave NP up\n",
    "gave NP NP\n",
    "gave NP to NP\n",
    "```\n",
    "##### a. Use this method to study the complementation patterns of a verb of interest, and write suitable grammar productions. (This task is sometimes called lexical acquisition.)\n",
    "##### b. Identify some English verbs that are near-synonyms, such as the dumped/filled/loaded example from earlier in this chapter. Use the chunking method to study the complementation patterns of these verbs. Create a grammar to cover these cases. Can the verbs be freely substituted for each other, or are their constraints? Discuss your findings.\n",
    "#### 33.  Develop a left-corner parser based on the recursive descent parser, and inheriting from ParseI.\n",
    "#### 34. Extend NLTK's shift-reduce parser to incorporate backtracking, so that it is guaranteed to find all parses that exist (i.e. it is complete).\n",
    "#### 35. Modify the functions `init_wfst()` and `complete_wfst()` so that when a non-terminal symbol is added to a cell in the WFST, it includes a record of the cells from which it was derived. Implement a function that will convert a WFST in this form to a parse tree."
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}